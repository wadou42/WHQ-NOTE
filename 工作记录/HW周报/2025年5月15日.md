### 背景
之前使用我们的方法以及SRTuner和RandomTuner对scann和doris进行测试，得到的结果并不理想。在各种方法上，相对于-O3的提高均不高于5%

### 近期工作
**实验一**：验证现在的搜索空间是否可以相对于-O3提升5%

现有的搜索空间是所有 -O3 打开的opt（只选择01类型）, 我们在现有的搜索空间下对scann进行测试结果如下，其中SRTUner搜索了200轮，RandomTuner搜索180轮：
![[Pasted image 20250515160921.png]]
![[Randon_scann_170.png]]

**实验二**：测试我们的模型是否可以捕获到优秀的优化配置

1. 将一些表现比较好的opt加入到训练集中：
	从SCANN历史测试结果中（比如刚才的测试结果）找到一些表现较好的OPT，使用这些OPT对cBench进行测试，得到一些新的数据。将这些数据加入到原来训练集中进行模型训练，得到新模型，然后使用新模型预测得到TOP-10，再从TOP-10为起点进行搜索。

	向训练集中新增加入了61条opt，加上已有的共计1400+条opt。新模型预测得到的**TOP-10**中有**6条**是来自这些新增的opt。但是其中，有两条是在-O3的基础上更改1-3个优化选项。其余的则相对于-O3更改10-50个优化选项。

	说明模型对于优秀的优化配置选择能力还是可以的，之前的数据集则存在正负样本不均衡的情况。

2. 对于新的TOP-10 进行实际测试：
	测试结果如图，两条折线是对同一批opt进行两次测试的结果，误差还是较大的：![[scann两次实测.png]]

	这些点opts中，有6个是来自新增加的opt中，在迭代测试的时候是超过-O3的。但是有些再重新测试时却没有高于-O3， 属于程序的波动。 
	
	之前在测试的过程中采取的方法是在每一轮的迭代中（也就是测试每一个opt时）同时测试opt和-O3，然后再计算加速比。这样可以尽可能的减小误差，但是在一些特殊情况下也会放大误差，比如刚好-O3性能向下波动，opt的性能向上波动，就会出现“误报”， 认为这个opt的性能高于-O3。

新测试方法：

**中位数**  or 
**函数级别**

对于-O3进行测试，重复测试 n 次取均值，得到 -O3性能，此值保持固定不变，作为-O3的最终结果。

在测试时，以这个值作为-O3的性能进行测试。 最后对于我们得到最优的opt同样进行n次测试，得到最优opt的性能，计算最终的加速比。


### Doing & TODO
通过上述两个实验，我们发现在现有的搜索空间下，将程序性能提升5% 较为困难，因此目前正在做扩充搜索空间（目前有两个空间在做，一个是gcc所有的优化选项，一个是-Ofast的优化选项）。 扩充主要从两个部分，一是在模型训练，按照新的搜索空间进行重新收集数据训练模型。二是在搜索迭代中，除了01外，也考虑数值型的。

我在做第一部分，这次重新收集数据时额外考虑两点，一是正负样本平衡，二是额外考虑一些cpu密集型的程序，因为提升空间更大一些。

**下一步计划：**
* 完成新的数据集收集

* 在模型的训练中，使用 “对比学习” 对优化配置进行预处理 （待实现）
	* 因为在收集数据时发现，很多opt几乎一直保持开或者关闭的状态。可以通过对比学习，忽略到这些维，降低向量的维度。

* 使用新的模型进行测试训练
	* 前文提到的新的测试流程——固定-O3性能
	* 尝试不同的 “由函数级的优化选项聚合到应用级的优化选项“ 的投票方法