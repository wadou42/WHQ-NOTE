## 方案介绍
方案主要分为两个部分，基于代码表征的预测模块与基于预测结果的搜索模块。其中，基于代码表征的预测模块通过预测得出比较优秀的一组优化配置；搜索部分以此为起点进行局部搜索，得到最优的优化配置。

### 基于代码表征的预测模块
**模型微调**
具体的，该模块采用CodeT5的一个变种——`T5ForCondictionGeneration`作为代码表征与二分类模型。将 ​**​函数代码 + 优化选项组合​**​ 作为输入，预测该优化组合是否比 `-O3` 更优（输出 `1` 表示优于 `-O3`，`0` 表示不优于）

我们构造的输入是一个自然语言提示（prompt）形式的文本，将**优化配置（config)** 和**函数代码（code）** 统一编码：

```python
input_text = ( 
	"Strictly classify if this optimization is better than -O3. If uncertain, answer '0'.\n" 
	f"Optimization:\n{config}\n" 
	f"Code:\n{code}" 
)
```

其中：
[1,0,1,1]
**在cpu时间**
- **config** 是优化选项的 one-hot 编码（通过 `<opt_42_on>` 或 `<opt_42_off>` 的 token 表示第42个选项开或者关闭），并按字典序排列，增加结构一致性。

* **config** 是删除不重要选项之后的选项组合
    
- 将 `<opt_XX_on/off>` token 加入 T5 的词表，确保模型理解它们的语义，并且减少优化选项占用的token数。
	```python
	opt_on_tokens = [f"<opt_on_{idx}>" for idx in range(263)]
	opt_off_tokens = [f"<opt_off_{idx}>" for idx in range(263)]
	
	self.tokenizer.add_tokens(opt_on_tokens)
	self.tokenizer.add_tokens(opt_off_tokens)
	
	model.resize_token_embeddings(len(self.tokenizer))	
	```
    
- 模型使用 **`T5ForConditionalGeneration`** 微调，以学习从输入的代码+优化配置到“是否优于 -O3”的映射，使用 T5 的 `encoder-decoder` 架构生成 **0 或 1** 的预测输出，表示是否优于 `-O3`。

**基于模型预测**

**总结：按照函数的执行时间作为权重，对置信度进行加权平均。**

我们首先从训练集中提取了 6,000 多条优化配置（每条配置是多个编译器优化选项的组合）。同时，从待调优的目标程序中提取热点函数，作为评估对象。

然后我们构造所有“优化配置 × 热点函数”的组合，输入到已微调的 T5ForCondictionGeneration 预测模型中，模型会输出每个组合在该配置下是否优于 `-O3` 的概率（即置信度）。

对于每个优化配置，我们对其在所有热点函数上的预测置信度进行加权平均，权重为热点函数的执行时间（越耗时越重要）。这个加权平均值用于衡量该配置的整体潜力。

最后，我们根据加权得分对所有配置进行排序，选取前 10 个置信度最高的配置，作为搜索模块的搜索起点，用于后续的性能调优过程。

**为何选择T5ForCondictiongeneration？**
1. T5ForCondictiongeneration 类模型是为**文本到文本**任务设计的通用模型，能很好地理解上下文中的结构性自然语言提示，也容易理解代码的语义信息。

2. T5 模型允许我们向分词器中加入自定义的 token，例如 `<opt_42_on>`、`<opt_42_off>` 等，并在微调过程中共同学习这些 token 的语义。这种机制使模型能够更深入地理解优化选项的结构和含义。虽然 BERT 类模型在技术上也支持加入自定义 token，但由于其 encoder-only 的架构和预训练方式限制，这些 token 的语义更难在微调中充分学习；而 T5 模型的 seq2seq 结构更适合表达结构化输入和 prompt，并能更有效地融合这些 token 的含义。

3. T5 采用 seq2seq 架构，我们预测 `0` 或 `1`，将来如果需要扩展为多分类任务（如输出 `0` / `1` / `2`，或解释性文字），T5 的 decoder 可以直接生成目标序列，而无需结构修改。

### 搜索模块
以预测模块的结果为基础，进行局部搜索。

**约束预处理**
我们采用的搜索空间是gcc所有的0-1选项，但大部分程序无法在所有的优化组合下编译、运行成功，我们将无法编译、运行的最小优化组合成为约束。 

为了解决这一问题，我们基于z3约束求解器，实现了约束自动求解工具，可以实现约束的自动求解。

**搜索**
1. ​**​PSO（粒子群优化算法）​**​：模拟鸟群觅食行为，通过粒子协作在解空间中搜索最优解，适用于连续优化问题。
2. ​**​GA（遗传算法）​**​：模仿生物进化机制，通过选择、交叉、变异操作迭代优化解，适用于复杂非线性问题
3. ​**​SA（模拟退火算法）​**​：受金属退火过程启发，允许接受次优解以跳出局部最优，适合组合优化问题
4. ​**​DE（差分进化算法）**：基于群体差异的实数编码算法，通过差分变异和贪婪选择高效搜索全局最优解，尤其擅长高维问题

## 评估与测试
### `-O3`
1. redis
由于redis是单线程程序，所以其执行效率与在哪个cpu核执行有很大的关系，因此为了避免该因素的影响，redis-server在319核上运行，redis-benchmark在318核上运行。

重复100次执行，选择中位数作为最终的测试结果，最终redis在`-O3`执行时间为18.7741ms

2. doris
限制128核，重复100次执行，选择中位数作为最终的测试结果，最终doris在`-O3`执行时间为2519.5ms

3. scann
限制在128核，重复100次执行，选择中位数作为最终的测试结果，最终scann在`-O3`qps为 457.586

### SRTuner & Our Method
**SRTuner**：与测试-O3时采用同样的环境，测试限制在测试时间24小时，迭代轮次50轮。在测试Redis时，**每轮测试15遍**；Doris，**每轮测试15遍**；scann**每轮测试3遍**，从而在限定时间下最大程度保证搜索的准确性。测试使用的搜索空间为其自带的**gcc_opt.txt**。 在此基础上，我们排除了一些常见的约束，例如当`-ftoplevel-reorder`关闭的时候，`-fsection-anchors `必须关闭，然后进行迭代搜索，降低编译失败的概率。在搜索过程中，-O3的执行时间默认不变为前面测试的结果。

1. redis
	由于我们想测试多种不同的搜索算法，所以在我们的方法迭代搜索过程中，每次只测试1遍。但**最终结果测试了50遍**，可以确保结果的真实性。，我们的方法得到的加速比是**1.100**，SRTuner的加速比为**1.042**. 
		![[Pasted image 20250709150117.png|675]]
1. scann	
	同样Scann采用相同的方法，在迭代搜索的过程中，我们的方法只测试一遍，**最终测试3遍**。我们的方法加速比为**1.026**，SRTuner的加速比为**1.009**
	![[Pasted image 20250709150835.png|675]]
	scann统计：在以往测试的数据中，共计迭代1345次，其中无效迭代446次，优于`-O3`61次，优于`-O3` 3% 16    最高在4%-5%，但是由于只测试一次，所以无法并不一定真的可以复现。
	
3. doris
	约束问题依然存在....，之前找约束一直在147云服务器上找的，但是无法迁移到新的服务器上。因此这周没有进行doris实际测试。
			![[Pasted image 20250709155823.png|625]]
encoder + decoder
ir2vec + MLP / **decoder**
### 剩余工作
* [ ] 我们的方法在doris上的测试
* [ ] ir2vec基线测试

**问题**

第二阶段： 这个月进行汇报
平均下来5%  
mysql 
scann提升
ir2vec 是接T5还是MLP